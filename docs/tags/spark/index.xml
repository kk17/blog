<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on Code Play</title>
    <link>https://blog.kk17.net/tags/spark/</link>
    <description>Recent content in spark on Code Play</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 16 Jul 2019 19:31:49 +0800</lastBuildDate>
    
	<atom:link href="https://blog.kk17.net/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Spark Run Mode and Application Deployment Mode</title>
      <link>https://blog.kk17.net/post/spark-run-mode-and-application-deployment-mode/</link>
      <pubDate>Tue, 16 Jul 2019 19:31:49 +0800</pubDate>
      
      <guid>https://blog.kk17.net/post/spark-run-mode-and-application-deployment-mode/</guid>
      <description>Spark running mode is often be confused with application deploy mode.
Spark Running Mode Spark can run on a single local machine or on a cluster manager like Mesos or YARN to leverage the resources(memory, CPU, and so on) across the cluster.
Run Locally In local mode, spark jobs run on a single machine and are executed in parallel using multi-threading: this restricts parallelism to (at most) the number of cores in your machine.</description>
    </item>
    
    <item>
      <title>How to Extend Spark</title>
      <link>https://blog.kk17.net/post/how-to-extend-spark/</link>
      <pubDate>Sat, 25 May 2019 17:43:29 +0800</pubDate>
      
      <guid>https://blog.kk17.net/post/how-to-extend-spark/</guid>
      <description>&lt;p&gt;In this post, we go through extending a Spark application and also Spark APIs by some examples. These two kinds of extensions are sometimes related, and we go with extending a Spark application first.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Pitfalls in Using Spark Catalog API</title>
      <link>https://blog.kk17.net/post/pitfalls-in-using-spark-catalog-api/</link>
      <pubDate>Sat, 27 Apr 2019 16:42:59 +0800</pubDate>
      
      <guid>https://blog.kk17.net/post/pitfalls-in-using-spark-catalog-api/</guid>
      <description>&lt;p&gt;Spark SQL supports querying data via SQL and in order to use this feature, we must enable Spark with Hive support, because Spark uses Hive Metastore to store metadata. By default, Spark uses an in-memory embedded database called Derby to store the metadata, but it can also configure to use an external Hive Metastore. Spark Hive Configuration can be found here:  &lt;a href=&#34;https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html&#34;&gt;Hive Tables - Spark 2.4.1 Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Except using DDL SQL to manipulate metadata stored in Hive Metastore, Spark SQL also provides a minimalist API know as Catalog API to manipulate metadata in spark applications. Spark Catalog API can be found here: &lt;a href=&#34;https://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/catalog/Catalog.html&#34;&gt;Catalog (Spark 2.2.1 JavaDoc)&lt;/a&gt; and &lt;a href=&#34;https://spark.apache.org/docs/2.3.0/api/python/_modules/pyspark/sql/catalog.html&#34;&gt;pyspark.sql.catalog â€” PySpark master documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After using Spark Catalog API for a period of time, I found some pitfalls.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Use Spark Sql With Hive Metastore</title>
      <link>https://blog.kk17.net/post/use-spark-sql-with-hive-metastore/</link>
      <pubDate>Wed, 27 Feb 2019 15:52:48 +0800</pubDate>
      
      <guid>https://blog.kk17.net/post/use-spark-sql-with-hive-metastore/</guid>
      <description>Spark SQL Spark SQL is a module for working with structured data
spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate() spark.sql(&amp;quot;use database_name&amp;quot;) spark.sql(&amp;quot;select * from table_name where id in (3085,3086,3087) and dt &amp;gt; &amp;#39;2019-03-10&amp;#39;&amp;quot;).toPandas()  Hive Data warehouse software for reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.
Hive transforms SQL queries as Hive MapReduce job.
Besides MapReduce, Hive can use Spark and Tez as execute engine.</description>
    </item>
    
  </channel>
</rss>